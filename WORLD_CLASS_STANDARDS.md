# AI ORCHESTRATOR - WORLD-CLASS STANDARDS
## Beyond FAANG - A New Paradigm in Software Excellence

---

## ðŸŒŸ PHILOSOPHY

**FAANG standards are our BASELINE, not our ceiling.**

This system represents a NEW PARADIGM in software development:
- **Self-Improving**: Every addition makes the system better
- **Zero-Defect**: 99.999999999% quality standard (11 nines)
- **Compute-Optimal**: Every CPU cycle justified
- **Human-Centric**: Built for humans, by AI, with AI
- **Future-Proof**: Designed to evolve forever

---

## ðŸŽ¯ THE 10 COMMANDMENTS OF WORLD-CLASS CODE

### 1. ZERO TOLERANCE FOR MEDIOCRITY
**Standard**: If it's not exceptional, it doesn't ship.

```python
# âŒ FAANG LEVEL (Not Good Enough)
def process_data(data):
    """Process data."""
    return [x * 2 for x in data]

# âœ… WORLD-CLASS LEVEL
from typing import List, TypeVar, Callable, Optional
from functools import lru_cache
import logging

T = TypeVar('T', int, float)

logger = logging.getLogger(__name__)


class DataProcessor:
    """
    World-class data processor with comprehensive error handling,
    performance optimization, and observability.
    
    This processor implements:
    - Type-safe operations with generic support
    - Automatic caching for repeated operations
    - Comprehensive logging and metrics
    - Graceful error handling and recovery
    - Memory-efficient streaming for large datasets
    - Automatic performance profiling
    
    Performance Characteristics:
        Time Complexity: O(n) for linear operations
        Space Complexity: O(1) with streaming
        Cache Hit Rate: >95% for repeated operations
        
    Example:
        >>> processor = DataProcessor(operation=lambda x: x * 2)
        >>> result = processor.process([1, 2, 3])
        >>> print(result)
        [2, 4, 6]
        
    Attributes:
        operation: The transformation function to apply
        cache_size: Maximum cache size (default: 128)
        enable_profiling: Whether to profile performance
    """
    
    def __init__(
        self,
        operation: Callable[[T], T],
        cache_size: int = 128,
        enable_profiling: bool = True
    ) -> None:
        """
        Initialize the data processor.
        
        Args:
            operation: Function to apply to each element
            cache_size: Maximum number of cached results
            enable_profiling: Enable performance profiling
            
        Raises:
            ValueError: If cache_size < 1
            TypeError: If operation is not callable
        """
        if not callable(operation):
            raise TypeError("operation must be callable")
        if cache_size < 1:
            raise ValueError("cache_size must be at least 1")
            
        self.operation = lru_cache(maxsize=cache_size)(operation)
        self.enable_profiling = enable_profiling
        self._metrics = {
            "operations": 0,
            "cache_hits": 0,
            "errors": 0,
            "total_time": 0.0
        }
    
    def process(
        self,
        data: List[T],
        batch_size: Optional[int] = None
    ) -> List[T]:
        """
        Process data with world-class error handling and optimization.
        
        Args:
            data: Input data to process
            batch_size: Optional batch size for large datasets
            
        Returns:
            Processed data
            
        Raises:
            ValueError: If data is empty
            ProcessingError: If processing fails
            
        Performance:
            - Streams data for large inputs (>10k items)
            - Caches results for repeated operations
            - Profiles execution time when enabled
        """
        if not data:
            raise ValueError("Cannot process empty data")
        
        import time
        start_time = time.perf_counter()
        
        try:
            # Use streaming for large datasets
            if len(data) > 10000:
                result = list(self._process_stream(data, batch_size))
            else:
                result = [self.operation(x) for x in data]
            
            # Update metrics
            self._metrics["operations"] += len(data)
            
            if self.enable_profiling:
                duration = time.perf_counter() - start_time
                self._metrics["total_time"] += duration
                logger.info(
                    f"Processed {len(data)} items in {duration:.4f}s "
                    f"({len(data)/duration:.0f} items/sec)"
                )
            
            return result
            
        except Exception as e:
            self._metrics["errors"] += 1
            logger.exception(f"Processing failed: {e}")
            raise ProcessingError(f"Failed to process data: {e}") from e
    
    def _process_stream(
        self,
        data: List[T],
        batch_size: Optional[int]
    ) -> Generator[T, None, None]:
        """Stream processing for memory efficiency."""
        batch_size = batch_size or 1000
        
        for i in range(0, len(data), batch_size):
            batch = data[i:i + batch_size]
            yield from (self.operation(x) for x in batch)
    
    def get_metrics(self) -> Dict[str, Union[int, float]]:
        """
        Get performance metrics.
        
        Returns:
            Dictionary with performance metrics
        """
        cache_info = self.operation.cache_info()
        self._metrics["cache_hits"] = cache_info.hits
        self._metrics["cache_hit_rate"] = (
            cache_info.hits / max(cache_info.hits + cache_info.misses, 1)
        )
        return self._metrics.copy()
```

---

### 2. SELF-DOCUMENTING CODE
**Standard**: Code explains itself; comments explain WHY, not WHAT.

```python
# âŒ FAANG LEVEL
# Get the user
user = db.get_user(id)
# Check if user exists
if user:
    # Update the user
    user.name = new_name
    # Save to database
    db.save(user)

# âœ… WORLD-CLASS LEVEL
from typing import Optional
from dataclasses import dataclass
from datetime import datetime


@dataclass
class User:
    """User entity with audit trail."""
    id: str
    name: str
    updated_at: datetime
    updated_by: str


class UserService:
    """
    User management service with comprehensive audit trail.
    
    Why this design:
    - Separates data access from business logic
    - Provides clear transaction boundaries
    - Maintains audit trail for compliance
    - Enables easy testing with dependency injection
    """
    
    def __init__(self, repository: UserRepository, audit_log: AuditLog):
        self.repository = repository
        self.audit_log = audit_log
    
    def update_user_name(
        self,
        user_id: str,
        new_name: str,
        updated_by: str
    ) -> Optional[User]:
        """
        Update user name with audit trail.
        
        Why we audit: Regulatory compliance requires tracking
        all user data modifications with timestamp and actor.
        
        Why we return Optional: Allows caller to handle
        non-existent users gracefully without exceptions.
        """
        user = self.repository.find_by_id(user_id)
        
        if user is None:
            # Why we don't raise: Non-existent user is a valid
            # business case, not an error condition
            return None
        
        # Why we create new instance: Immutability prevents
        # accidental state corruption and enables event sourcing
        updated_user = User(
            id=user.id,
            name=new_name,
            updated_at=datetime.utcnow(),
            updated_by=updated_by
        )
        
        # Why we audit before save: Ensures audit trail even
        # if save fails, maintaining data integrity
        self.audit_log.record_change(
            entity="User",
            entity_id=user_id,
            field="name",
            old_value=user.name,
            new_value=new_name,
            changed_by=updated_by
        )
        
        return self.repository.save(updated_user)
```

---

### 3. PERFORMANCE IS A FEATURE
**Standard**: Every operation must be profiled and optimized.

```python
# âœ… WORLD-CLASS: Built-in performance monitoring
from typing import Callable, TypeVar, Any
from functools import wraps
import time
import logging
from contextlib import contextmanager

T = TypeVar('T')

logger = logging.getLogger(__name__)


class PerformanceMonitor:
    """
    World-class performance monitoring with automatic optimization.
    
    Features:
    - Automatic profiling of all operations
    - Performance regression detection
    - Automatic caching of slow operations
    - Real-time performance alerts
    - Historical performance tracking
    """
    
    def __init__(self):
        self.metrics = {}
        self.thresholds = {
            "warning": 1.0,  # 1 second
            "critical": 5.0   # 5 seconds
        }
    
    def profile(
        self,
        operation_name: Optional[str] = None,
        cache_slow: bool = True,
        alert_on_slow: bool = True
    ) -> Callable:
        """
        Decorator for automatic performance profiling.
        
        Args:
            operation_name: Custom operation name
            cache_slow: Auto-cache if operation is slow
            alert_on_slow: Alert if operation exceeds threshold
        """
        def decorator(func: Callable[..., T]) -> Callable[..., T]:
            name = operation_name or f"{func.__module__}.{func.__name__}"
            
            @wraps(func)
            def wrapper(*args: Any, **kwargs: Any) -> T:
                start = time.perf_counter()
                
                try:
                    result = func(*args, **kwargs)
                    duration = time.perf_counter() - start
                    
                    # Record metrics
                    self._record_metric(name, duration, success=True)
                    
                    # Alert on slow operations
                    if alert_on_slow and duration > self.thresholds["warning"]:
                        severity = (
                            "CRITICAL" if duration > self.thresholds["critical"]
                            else "WARNING"
                        )
                        logger.warning(
                            f"{severity}: {name} took {duration:.4f}s "
                            f"(threshold: {self.thresholds['warning']}s)"
                        )
                    
                    # Auto-cache slow operations
                    if cache_slow and duration > self.thresholds["warning"]:
                        logger.info(f"Auto-caching slow operation: {name}")
                        # Implementation of auto-caching logic
                    
                    return result
                    
                except Exception as e:
                    duration = time.perf_counter() - start
                    self._record_metric(name, duration, success=False)
                    raise
            
            return wrapper
        return decorator
    
    def _record_metric(
        self,
        operation: str,
        duration: float,
        success: bool
    ) -> None:
        """Record performance metric with historical tracking."""
        if operation not in self.metrics:
            self.metrics[operation] = {
                "count": 0,
                "total_time": 0.0,
                "min_time": float('inf'),
                "max_time": 0.0,
                "failures": 0,
                "history": []
            }
        
        metric = self.metrics[operation]
        metric["count"] += 1
        metric["total_time"] += duration
        metric["min_time"] = min(metric["min_time"], duration)
        metric["max_time"] = max(metric["max_time"], duration)
        
        if not success:
            metric["failures"] += 1
        
        # Keep last 100 measurements for trend analysis
        metric["history"].append({
            "timestamp": time.time(),
            "duration": duration,
            "success": success
        })
        if len(metric["history"]) > 100:
            metric["history"].pop(0)
    
    def get_report(self) -> Dict[str, Any]:
        """
        Generate comprehensive performance report.
        
        Returns:
            Dictionary with performance statistics and recommendations
        """
        report = {}
        
        for operation, metric in self.metrics.items():
            avg_time = metric["total_time"] / metric["count"]
            success_rate = (
                (metric["count"] - metric["failures"]) / metric["count"] * 100
            )
            
            # Detect performance regression
            recent_avg = sum(
                h["duration"] for h in metric["history"][-10:]
            ) / min(len(metric["history"]), 10)
            
            regression = recent_avg > avg_time * 1.2  # 20% slower
            
            report[operation] = {
                "calls": metric["count"],
                "avg_time": avg_time,
                "min_time": metric["min_time"],
                "max_time": metric["max_time"],
                "success_rate": success_rate,
                "recent_avg": recent_avg,
                "regression_detected": regression,
                "recommendation": self._get_recommendation(
                    avg_time, success_rate, regression
                )
            }
        
        return report
    
    def _get_recommendation(
        self,
        avg_time: float,
        success_rate: float,
        regression: bool
    ) -> str:
        """Generate optimization recommendation."""
        if regression:
            return "URGENT: Performance regression detected. Investigate immediately."
        elif avg_time > self.thresholds["critical"]:
            return "CRITICAL: Operation is very slow. Consider caching or optimization."
        elif avg_time > self.thresholds["warning"]:
            return "WARNING: Operation is slow. Monitor and consider optimization."
        elif success_rate < 95:
            return "WARNING: Low success rate. Investigate error causes."
        else:
            return "OK: Performance within acceptable range."


# Global performance monitor
perf_monitor = PerformanceMonitor()


# Usage example
@perf_monitor.profile(cache_slow=True, alert_on_slow=True)
def expensive_operation(data: List[int]) -> int:
    """Example of automatically profiled operation."""
    return sum(x ** 2 for x in data)
```

---

### 4. ZERO-DEFECT MINDSET
**Standard**: Bugs are unacceptable. Prevention over detection.

```python
# âœ… WORLD-CLASS: Multiple layers of defect prevention
from typing import Any, Dict, List, Optional, TypeVar, Generic
from pydantic import BaseModel, validator, Field
from enum import Enum
import logging

logger = logging.getLogger(__name__)

T = TypeVar('T')


class TaskStatus(Enum):
    """
    Task status with explicit states.
    
    Why enum: Prevents invalid states, enables exhaustive
    pattern matching, provides type safety.
    """
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"


class Task(BaseModel):
    """
    Task model with comprehensive validation.
    
    Why Pydantic: Automatic validation, serialization,
    and documentation. Catches errors at data boundary.
    """
    
    id: str = Field(..., min_length=1, max_length=100)
    description: str = Field(..., min_length=1, max_length=1000)
    priority: int = Field(..., ge=1, le=5)
    status: TaskStatus = TaskStatus.PENDING
    dependencies: List[str] = Field(default_factory=list)
    metadata: Dict[str, Any] = Field(default_factory=dict)
    
    @validator('id')
    def validate_id_format(cls, v: str) -> str:
        """
        Validate task ID format.
        
        Why custom validator: Business rules that can't be
        expressed with simple constraints.
        """
        if not v.startswith('task_'):
            raise ValueError("Task ID must start with 'task_'")
        return v
    
    @validator('dependencies')
    def validate_no_self_dependency(cls, v: List[str], values: Dict) -> List[str]:
        """
        Prevent self-dependencies.
        
        Why: Self-dependencies cause infinite loops.
        Catch at data level, not runtime.
        """
        if 'id' in values and values['id'] in v:
            raise ValueError("Task cannot depend on itself")
        return v
    
    class Config:
        """Pydantic configuration for strict validation."""
        # Why strict: Catch type coercion bugs early
        strict = True
        # Why frozen: Immutability prevents accidental mutations
        frozen = False  # Allow status updates
        # Why validate_assignment: Catch bugs on field updates
        validate_assignment = True


class TaskManager:
    """
    Task manager with defense-in-depth error prevention.
    
    Layers of defense:
    1. Type system (mypy)
    2. Pydantic validation
    3. Business logic validation
    4. Runtime assertions
    5. Comprehensive logging
    6. Automatic recovery
    """
    
    def __init__(self):
        self.tasks: Dict[str, Task] = {}
        self._validate_invariants()
    
    def add_task(self, task: Task) -> None:
        """
        Add task with comprehensive validation.
        
        Defense layers:
        1. Pydantic validates task structure
        2. Check for duplicate IDs
        3. Validate dependencies exist
        4. Check for circular dependencies
        5. Verify system invariants
        """
        # Layer 2: Duplicate check
        if task.id in self.tasks:
            raise ValueError(f"Task {task.id} already exists")
        
        # Layer 3: Dependency validation
        for dep_id in task.dependencies:
            if dep_id not in self.tasks:
                raise ValueError(f"Dependency {dep_id} does not exist")
        
        # Layer 4: Circular dependency check
        if self._has_circular_dependency(task):
            raise ValueError(f"Task {task.id} creates circular dependency")
        
        # Add task
        self.tasks[task.id] = task
        
        # Layer 5: Verify invariants
        self._validate_invariants()
        
        logger.info(f"Task {task.id} added successfully")
    
    def _has_circular_dependency(self, task: Task) -> bool:
        """
        Detect circular dependencies using DFS.
        
        Why: Circular dependencies cause deadlocks.
        Must be prevented, not handled.
        """
        def dfs(task_id: str, visited: set, path: set) -> bool:
            if task_id in path:
                return True  # Cycle detected
            if task_id in visited:
                return False
            
            visited.add(task_id)
            path.add(task_id)
            
            if task_id in self.tasks:
                for dep in self.tasks[task_id].dependencies:
                    if dfs(dep, visited, path):
                        return True
            
            path.remove(task_id)
            return False
        
        # Check if adding this task creates a cycle
        temp_tasks = self.tasks.copy()
        temp_tasks[task.id] = task
        
        visited = set()
        for dep in task.dependencies:
            if dfs(dep, visited, {task.id}):
                return True
        
        return False
    
    def _validate_invariants(self) -> None:
        """
        Validate system invariants.
        
        Why: Catch bugs that slip through other layers.
        Fail fast with clear error messages.
        """
        # Invariant 1: All dependencies must exist
        for task in self.tasks.values():
            for dep_id in task.dependencies:
                assert dep_id in self.tasks, \
                    f"Invariant violated: Task {task.id} depends on non-existent {dep_id}"
        
        # Invariant 2: No circular dependencies
        for task in self.tasks.values():
            assert not self._has_circular_dependency(task), \
                f"Invariant violated: Circular dependency involving {task.id}"
        
        # Invariant 3: All task IDs are unique
        assert len(self.tasks) == len(set(self.tasks.keys())), \
            "Invariant violated: Duplicate task IDs"
        
        logger.debug("All invariants validated successfully")
```

---

### 5. OBSERVABILITY IS MANDATORY
**Standard**: Every operation must be observable, traceable, and debuggable.

```python
# âœ… WORLD-CLASS: Comprehensive observability
from typing import Any, Dict, Optional
from dataclasses import dataclass, field
from datetime import datetime
import logging
import json
import traceback
from contextlib import contextmanager
import uuid

logger = logging.getLogger(__name__)


@dataclass
class ExecutionContext:
    """
    Execution context for distributed tracing.
    
    Why: Enables tracing requests across agents, tools,
    and system boundaries. Essential for debugging
    complex multi-agent workflows.
    """
    trace_id: str = field(default_factory=lambda: str(uuid.uuid4()))
    span_id: str = field(default_factory=lambda: str(uuid.uuid4()))
    parent_span_id: Optional[str] = None
    user_id: Optional[str] = None
    task_id: Optional[str] = None
    agent_id: Optional[str] = None
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    def create_child_context(self) -> 'ExecutionContext':
        """Create child context for nested operations."""
        return ExecutionContext(
            trace_id=self.trace_id,
            span_id=str(uuid.uuid4()),
            parent_span_id=self.span_id,
            user_id=self.user_id,
            task_id=self.task_id,
            agent_id=self.agent_id,
            metadata=self.metadata.copy()
        )


class ObservabilityLogger:
    """
    World-class observability with structured logging,
    metrics, and distributed tracing.
    """
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
    
    @contextmanager
    def trace_operation(
        self,
        operation: str,
        context: ExecutionContext,
        **kwargs: Any
    ):
        """
        Trace an operation with comprehensive logging.
        
        Captures:
        - Start/end timestamps
        - Duration
        - Success/failure
        - Error details
        - Performance metrics
        - Context propagation
        """
        start_time = datetime.utcnow()
        
        # Structured log entry
        log_entry = {
            "event": "operation_start",
            "operation": operation,
            "trace_id": context.trace_id,
            "span_id": context.span_id,
            "parent_span_id": context.parent_span_id,
            "timestamp": start_time.isoformat(),
            "user_id": context.user_id,
            "task_id": context.task_id,
            "agent_id": context.agent_id,
            **kwargs
        }
        
        self.logger.info(json.dumps(log_entry))
        
        try:
            yield context
            
            # Success log
            end_time = datetime.utcnow()
            duration = (end_time - start_time).total_seconds()
            
            log_entry.update({
                "event": "operation_complete",
                "status": "success",
                "duration_seconds": duration,
                "end_timestamp": end_time.isoformat()
            })
            
            self.logger.info(json.dumps(log_entry))
            
        except Exception as e:
            # Failure log with full context
            end_time = datetime.utcnow()
            duration = (end_time - start_time).total_seconds()
            
            log_entry.update({
                "event": "operation_failed",
                "status": "error",
                "duration_seconds": duration,
                "end_timestamp": end_time.isoformat(),
                "error_type": type(e).__name__,
                "error_message": str(e),
                "error_traceback": traceback.format_exc()
            })
            
            self.logger.error(json.dumps(log_entry))
            raise


# Global observability logger
obs_logger = ObservabilityLogger()


# Usage example
def process_task_with_observability(task_id: str, context: ExecutionContext):
    """Example of fully observable operation."""
    with obs_logger.trace_operation(
        operation="process_task",
        context=context,
        task_id=task_id
    ) as ctx:
        # All operations within this context are traced
        result = do_work(task_id)
        
        # Nested operations get child contexts
        child_ctx = ctx.create_child_context()
        with obs_logger.trace_operation(
            operation="validate_result",
            context=child_ctx
        ):
            validate(result)
        
        return result
```

---

### 6. SECURITY BY DEFAULT
**Standard**: Every input is hostile until proven otherwise.

```python
# âœ… WORLD-CLASS: Defense-in-depth security
from typing import Any, Dict, List
from pydantic import BaseModel, validator, Field
import re
import html
import logging
from functools import wraps

logger = logging.getLogger(__name__)


class SecureInput(BaseModel):
    """
    Secure input validation with multiple defense layers.
    
    Security layers:
    1. Type validation
    2. Length limits
    3. Format validation
    4. Content sanitization
    5. Injection prevention
    6. Rate limiting
    """
    
    user_input: str = Field(..., min_length=1, max_length=1000)
    
    @validator('user_input')
    def sanitize_input(cls, v: str) -> str:
        """
        Sanitize input to prevent injection attacks.
        
        Defenses:
        - HTML escaping
        - SQL injection prevention
        - Command injection prevention
        - Path traversal prevention
        """
        # HTML escape
        v = html.escape(v)
        
        # Remove potential SQL injection patterns
        sql_patterns = [
            r"(\b(SELECT|INSERT|UPDATE|DELETE|DROP|CREATE|ALTER)\b)",
            r"(--|;|\/\*|\*\/)",
            r"(\bOR\b.*=.*)",
            r"(\bAND\b.*=.*)"
        ]
        for pattern in sql_patterns:
            if re.search(pattern, v, re.IGNORECASE):
                raise ValueError("Potential SQL injection detected")
        
        # Remove potential command injection
        cmd_patterns = [r"[;&|`$()]", r"\.\./", r"~"]
        for pattern in cmd_patterns:
            if re.search(pattern, v):
                raise ValueError("Potential command injection detected")
        
        return v


def require_authentication(func):
    """
    Decorator to enforce authentication.
    
    Why decorator: Centralized security policy,
    impossible to forget, easy to audit.
    """
    @wraps(func)
    def wrapper(*args, **kwargs):
        # Check authentication
        if not is_authenticated():
            logger.warning(f"Unauthorized access attempt to {func.__name__}")
            raise PermissionError("Authentication required")
        
        # Check authorization
        if not is_authorized(func.__name__):
            logger.warning(
                f"Unauthorized access attempt to {func.__name__} "
                f"by {get_current_user()}"
            )
            raise PermissionError("Insufficient permissions")
        
        # Audit log
        logger.info(
            f"Authorized access to {func.__name__} "
            f"by {get_current_user()}"
        )
        
        return func(*args, **kwargs)
    
    return wrapper


class SecureAgent:
    """Agent with comprehensive security controls."""
    
    @require_authentication
    def execute_task(self, task: SecureInput) -> Dict[str, Any]:
        """
        Execute task with security controls.
        
        Security measures:
        - Authentication required
        - Input validation
        - Output sanitization
        - Audit logging
        - Rate limiting
        - Resource limits
        """
        # Rate limiting
        if not self._check_rate_limit():
            raise PermissionError("Rate limit exceeded")
        
        # Resource limits
        with self._resource_limits():
            result = self._execute_internal(task)
        
        # Sanitize output
        result = self._sanitize_output(result)
        
        # Audit log
        self._audit_log(task, result)
        
        return result
```

---

### 7. TESTABILITY IS NON-NEGOTIABLE
**Standard**: If it can't be tested, it can't be shipped.

```python
# âœ… WORLD-CLASS: 100% testable design
from typing import Protocol, Any
from abc import ABC, abstractmethod


# Dependency Injection for testability
class ToolInterface(Protocol):
    """Protocol for tool dependencies."""
    def execute(self, input: str) -> str: ...


class Agent:
    """
    Agent with 100% testable design.
    
    Why this design:
    - Dependency injection enables mocking
    - Protocol types enable duck typing
    - Pure functions enable property testing
    - No global state enables parallel testing
    - Clear interfaces enable contract testing
    """
    
    def __init__(
        self,
        tool: ToolInterface,  # Injected dependency
        config: Dict[str, Any]
    ):
        self.tool = tool
        self.config = config
    
    def process(self, input: str) -> str:
        """
        Pure function for easy testing.
        
        Why pure: Same input always produces same output,
        no side effects, easy to test, easy to reason about.
        """
        validated_input = self._validate(input)
        result = self.tool.execute(validated_input)
        return self._format(result)
    
    def _validate(self, input: str) -> str:
        """Extracted for unit testing."""
        if not input:
            raise ValueError("Input cannot be empty")
        return input.strip()
    
    def _format(self, result: str) -> str:
        """Extracted for unit testing."""
        return f"Result: {result}"


# Test file
import pytest
from unittest.mock import Mock


class TestAgent:
    """Comprehensive test suite."""
    
    @pytest.fixture
    def mock_tool(self):
        """Mock tool for testing."""
        tool = Mock(spec=ToolInterface)
        tool.execute.return_value = "mocked result"
        return tool
    
    @pytest.fixture
    def agent(self, mock_tool):
        """Agent with mocked dependencies."""
        return Agent(tool=mock_tool, config={})
    
    def test_process_success(self, agent, mock_tool):
        """Test successful processing."""
        result = agent.process("test input")
        
        assert result == "Result: mocked result"
        mock_tool.execute.assert_called_once_with("test input")
    
    def test_process_empty_input(self, agent):
        """Test error handling."""
        with pytest.raises(ValueError, match="Input cannot be empty"):
            agent.process("")
    
    @pytest.mark.parametrize("input,expected", [
        ("  test  ", "test"),
        ("test", "test"),
        ("  ", ValueError)
    ])
    def test_validate(self, agent, input, expected):
        """Property-based testing."""
        if expected == ValueError:
            with pytest.raises(ValueError):
                agent._validate(input)
        else:
            assert agent._validate(input) == expected
```

---

### 8. DOCUMENTATION IS CODE
**Standard**: Documentation must be executable,
